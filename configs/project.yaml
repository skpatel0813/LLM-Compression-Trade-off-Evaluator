# Central configuration for the headless pipeline

paths:
  data_dir: "data"
  outputs_dir: "outputs"
  reports_dir: "reports"
  lora_dir: "outputs/llama31_8b_kd_lora/lora"
  merged_dir: "outputs/llama31_8b_merged_fp16"
  gptq_dir: "outputs/llama31_8b_kd_gptq"

models:
  teacher_id: "meta-llama/Meta-Llama-3.1-70B-Instruct"
  student_id: "meta-llama/Meta-Llama-3.1-8B-Instruct"

training:
  seq_len: 4096
  epochs: 2
  per_device_batch_size: 1
  grad_accum: 16
  learning_rate: 2.0e-4
  kd_alpha: 0.5
  kd_temp: 1.0

quantization:
  bits: 4
  group_size: 128
  desc_act: true
  calib_examples: 256
  calib_batch_size: 8

evaluation:
  codebleu_limit: 200     # CodeSearchNet val samples to score
  latency_limit: 0        # 0 = all HumanEval items; set N to cap
  max_new_tokens: 256
